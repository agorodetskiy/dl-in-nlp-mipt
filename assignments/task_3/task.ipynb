{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results of this task**:\n",
    " * trained word vectors (mention somewhere, how long it took to train)\n",
    " * plotted loss (so we can see that it has converged)\n",
    " * function to map token to corresponding word vector\n",
    " * beautiful visualizations (PCE, T-SNE), you can use TensorBoard and play with your vectors in 3D (don't forget to add screenshots to the task)\n",
    " * qualitative evaluations of word vectors: nearest neighbors, word analogies\n",
    "\n",
    "**Extra:**\n",
    " * quantitative evaluation:\n",
    "   * for intrinsic evaluation you can find datasets [here](https://aclweb.org/aclwiki/Analogy_(State_of_the_art))\n",
    "   * for extrincis evaluation you can use [these](https://medium.com/@dataturks/rare-text-classification-open-datasets-9d340c8c508e)\n",
    "\n",
    "Also, you can find any other datasets for quantitative evaluation. If you chose to do this, please use the same datasets across tasks 3, 4, 5 and 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./dataset/corpus', 'rt') as fp:\n",
    "#     corpus = fp.read()\n",
    "\n",
    "# corpus = corpus.strip()\n",
    "# corpus = corpus.split()[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'he is a king',\n",
    "    'she is a queen',\n",
    "    'he is a man',\n",
    "    'she is a woman',\n",
    "    'warsaw is poland capital',\n",
    "    'berlin is germany capital',\n",
    "    'paris is france capital',\n",
    "]\n",
    "corpus = [w for doc in corpus for w in doc.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramBatcher:\n",
    "    def __init__(self, corpus, window_size=4, batch_size=32):\n",
    "        '''corpus - list of words'''\n",
    "        self.corpus = corpus\n",
    "        self.window_size = window_size\n",
    "        self.batch_size = batch_size\n",
    "        self.make_vocab()\n",
    "        return\n",
    "    \n",
    "    def make_vocab(self):\n",
    "        self.vocab = sorted(set(corpus))\n",
    "        self.V = len(self.vocab)\n",
    "        self.word2index = {w: idx for idx, w in enumerate(self.vocab)}\n",
    "        self.index2word = {idx: w for idx, w in enumerate(self.vocab)}\n",
    "        return\n",
    "    \n",
    "    def batch_gen(self):\n",
    "        '''c - corpus, v - vocab ; i - central, j - side'''\n",
    "        x_batch = np.empty(self.batch_size, dtype=np.int)\n",
    "        y_batch = np.empty(self.batch_size, dtype=np.int)\n",
    "        curr_idx = 0\n",
    "        for c_i, w in enumerate(self.corpus):\n",
    "            v_i = self.word2index[w]\n",
    "            window_left_idx = c_i - self.window_size if c_i - self.window_size >= 0 else 0\n",
    "            for side_w in self.corpus[window_left_idx: c_i] \\\n",
    "                          + self.corpus[c_i + 1 : c_i + self.window_size + 1]:\n",
    "                v_j = self.word2index[side_w]\n",
    "                x_batch[curr_idx] = v_i\n",
    "                y_batch[curr_idx] = v_j\n",
    "                curr_idx += 1\n",
    "                if curr_idx == self.batch_size:\n",
    "                    curr_idx = 0\n",
    "                    yield (x_batch, y_batch)\n",
    "        if curr_idx != 0:\n",
    "            yield (x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_iter = 3\n",
    "#for x_batch, y_batch in islice(batcher.batch_gen(), max_iter):\n",
    "    #print('x_batch.shape: {}, y_batch.shape: {}'.format(x_batch.shape, y_batch.shape))\n",
    "    #print(x_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 1\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(param_name, param_values, train_values, val_values):\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.plot(param_values, train_values, 'o-', label='train')\n",
    "    plt.plot(param_values, val_values, 'o-', label='val')\n",
    "\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel('metric')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(param_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "batcher = SkipGramBatcher(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = batcher.V\n",
    "N = 30\n",
    "batch_size = batcher.batch_size\n",
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential( torch.nn.Linear(V, N, bias=False),\n",
    "                             torch.nn.Linear(N, V, bias=False),\n",
    "                             torch.nn.LogSoftmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs=1):\n",
    "    \"\"\"    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    input_buff = np.empty((batch_size, V), dtype=np.float32)\n",
    "    #model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        loss_values = []\n",
    "        for t, (x_ids, y_ids) in enumerate(loader_train):\n",
    "            # print generated batches\n",
    "            #for elem in list(zip([batcher.index2word[elem] for elem in x_ids], [batcher.index2word[elem] for elem in y_ids])):\n",
    "                #print(elem[0], elem[1])\n",
    "\n",
    "            model.train()  # put model to training mode\n",
    "            #x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            #y = y.to(device=device, dtype=torch.long)\n",
    "            \n",
    "            input_buff.fill(0)\n",
    "            input_buff[np.arange(batch_size), x_ids] = 1\n",
    "\n",
    "            x = torch.from_numpy(input_buff)\n",
    "            y = torch.from_numpy(y_ids)\n",
    "            \n",
    "            scores = model(x)\n",
    "            #print(scores.shape)\n",
    "            #print('scores: {}\\ny: {}'.format(scores, y))\n",
    "            loss = torch.nn.functional.nll_loss(scores, y)\n",
    "            #loss_values.append(loss)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "        #plt.plot(loss_values)\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 5000\n",
    "loader_train = islice(batcher.batch_gen(), max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.7333\n",
      "Iteration 1, loss = 2.7209\n",
      "Iteration 2, loss = 2.7268\n",
      "Iteration 3, loss = 2.7136\n",
      "Iteration 4, loss = 2.7112\n",
      "Iteration 5, loss = 2.7452\n",
      "Iteration 6, loss = 2.7558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ainox/.local/lib/python3.7/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECK SOFTMAX DIMENSIONS!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
