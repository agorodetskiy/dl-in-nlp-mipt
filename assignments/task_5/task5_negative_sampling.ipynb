{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative sampling\n",
    "\n",
    "You may have noticed that word2vec is really slow to train. Especially with big (> 50 000) vocabularies. Negative sampling is the solution.\n",
    "\n",
    "The task is to implement word2vec with negative sampling. In more detail: you should implement two ways of negative sampling.\n",
    "\n",
    "## Vanilla negative sampling\n",
    "\n",
    "This is what was discussed in Stanford lecture. The main idea is in the formula:\n",
    "\n",
    "$$ L = \\log\\sigma(u^T_o u_c) + \\sum^k_{i=1} \\mathbb{E}_{j \\sim P(w)}[\\log\\sigma(-u^T_j, u_c)]$$\n",
    "\n",
    "Where $\\sigma$ - sigmoid function, $u_c$ - central word vector, $u_o$ - context (outside of the window) word vector, $u_j$ - vector or word with index $j$.\n",
    "\n",
    "The first term calculates the similarity between positive examples (word from one window)\n",
    "\n",
    "The second term is responsible for negative samples. $k$ is a hyperparameter - the number of negatives to sample.\n",
    "$\\mathbb{E}_{j \\sim P(w)}$\n",
    "means that $j$ is distributed accordingly to unigram distribution, but it is better to use $P^{3/4}(w)$ (empirical results) and you can experiment with some other approaches (for example, try to use uniform distribution).\n",
    "\n",
    "Thus, it is only required to calculate the similarity between positive samples and some other negatives. Not across all the vocabulary.\n",
    "\n",
    "Useful links:\n",
    "1. [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "1. [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "\n",
    "## Batch-transpose trick for negative sampling\n",
    "\n",
    "But we can do better. Maybe we don't need to compute vectors for negative samples at all, because we already have a batch of training data and (hopefully) examples in the batch are highly decorrelated.\n",
    "\n",
    "Let's assume we work with Skip-gram model.\n",
    "\n",
    "Let $S$ be a batch of _L2-normalized_ word vectors `(batch_size, 2*window_size + 1, word_vector_dim)`.\n",
    "\n",
    "```python\n",
    "x = 0.0\n",
    "for batch_idx in range(batch):\n",
    "    w = S[batch_idx, :, :]\n",
    "    x += np.sum(w.T @ w - 1.)\n",
    "\n",
    "y = 0.0\n",
    "for window_idx in range(window):\n",
    "    b = S[:, window_idx, :]\n",
    "    y += np.sum(b.T @ b)\n",
    "\n",
    "loss = -x + y```\n",
    "\n",
    "Think about this loss and compare it to vanilla negative sampling.\n",
    "\n",
    "Implement word2vec with batch-transpose trick. Modify the formula, if needed.\n",
    "\n",
    "If you are interested: [more info](https://www.tensorflow.org/extras/candidate_sampling.pdf) on other methods of candidate sampling.\n",
    "\n",
    "**Results of this task** are the very same as in task 3, **plus**:\n",
    " * implement two models (one with vanilla negative sampling and the other with batch-transpose trick)\n",
    " * compare all of the models from tasks 3-5. In terms of time and number of iterations until convergence and the quality of the resulting vectors.\n",
    " * answer the questions\n",
    "\n",
    "### Questions:\n",
    "1. Explain the batch-transpose trick formula in your own words. How would you name x, y, w and b?\n",
    "1. Should it be modified to serve as a word2vec loss? If yes, how?\n",
    "1. Is it possible to do the same trick with CBOW model? If yes, how?\n",
    "1. Does it matter how the batch is made in the case of batch-transpose trick? In the case of vanilla negative sampling?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers:\n",
    "  1. _\n",
    "  1. _\n",
    "  1. _\n",
    "  1. _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
